#!/usr/bin/env python3
"""
Simplified training demonstration with comprehensive learning curves
This creates all the visualization plots with sample data to show the functionality
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def create_demo_results():
    """Create demonstration training results and plots"""
    
    print("üöÄ Creating comprehensive training demonstration...")
    
    # Create output directories
    output_dir = "training_results"
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/plots", exist_ok=True)
    os.makedirs(f"{output_dir}/logs", exist_ok=True)
    
    # Simulate training data for 30 epochs
    epochs = list(range(1, 31))
    np.random.seed(42)
    
    # Generate realistic training curves with proper convergence
    base_train_loss = np.exp(-np.array(epochs) * 0.15) + 0.1 + np.random.normal(0, 0.02, 30)
    base_val_loss = np.exp(-np.array(epochs) * 0.12) + 0.15 + np.random.normal(0, 0.03, 30)
    
    train_accuracy = 1 - base_train_loss + np.random.normal(0, 0.01, 30)
    val_accuracy = 1 - base_val_loss + np.random.normal(0, 0.015, 30) 
    test_accuracy = val_accuracy + np.random.normal(0, 0.01, 30)
    
    # Ensure realistic bounds
    train_accuracy = np.clip(train_accuracy, 0.5, 0.99)
    val_accuracy = np.clip(val_accuracy, 0.5, 0.95)
    test_accuracy = np.clip(test_accuracy, 0.5, 0.95)
    
    # Generate detailed metrics
    train_precision = train_accuracy + np.random.normal(0, 0.02, 30)
    train_recall = train_accuracy + np.random.normal(0, 0.02, 30)
    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)
    
    val_precision = val_accuracy + np.random.normal(0, 0.02, 30)
    val_recall = val_accuracy + np.random.normal(0, 0.02, 30)
    val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall)
    
    test_precision = test_accuracy + np.random.normal(0, 0.02, 30)
    test_recall = test_accuracy + np.random.normal(0, 0.02, 30)
    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)
    
    # Clip all metrics to [0, 1]
    for arr in [train_precision, train_recall, train_f1, val_precision, val_recall, val_f1, 
                test_precision, test_recall, test_f1]:
        arr[:] = np.clip(arr, 0, 1)
    
    # Learning rate schedule
    learning_rates = [0.001 * (0.5 ** (epoch // 10)) for epoch in epochs]
    
    # Create comprehensive plots
    create_learning_curves(epochs, base_train_loss, base_val_loss, train_accuracy, 
                          val_accuracy, test_accuracy, learning_rates, output_dir)
    
    create_detailed_metrics_plot(epochs, train_precision, train_recall, train_f1,
                                val_precision, val_recall, val_f1,
                                test_precision, test_recall, test_f1, output_dir)
    
    create_comprehensive_analysis(epochs, base_train_loss, base_val_loss, train_accuracy,
                                 val_accuracy, test_accuracy, learning_rates,
                                 train_precision, train_recall, train_f1,
                                 val_precision, val_recall, val_f1,
                                 test_precision, test_recall, test_f1, output_dir)
    
    create_confusion_matrix(output_dir)
    create_roc_curve(output_dir)
    
    # Save training logs
    save_training_logs(epochs, base_train_loss, base_val_loss, train_accuracy,
                      val_accuracy, test_accuracy, learning_rates,
                      train_precision, train_recall, train_f1,
                      val_precision, val_recall, val_f1,
                      test_precision, test_recall, test_f1, output_dir)
    
    # Create dataset information
    create_dataset_info(output_dir)
    
    print(f"‚úÖ Training demonstration completed!")
    print(f"üìä Results saved to: {output_dir}/")
    print(f"üìà Learning curves: {output_dir}/plots/")
    print(f"üìù Training logs: {output_dir}/logs/")

def create_learning_curves(epochs, train_loss, val_loss, train_acc, val_acc, test_acc, lr, output_dir):
    """Create main learning curves plot"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Learning Curves - Dual-Branch CNN-BiLSTM-Autoencoder', fontsize=16, fontweight='bold')
    
    # Loss curves
    axes[0, 0].plot(epochs, train_loss, label='Training Loss', color='blue', linewidth=2)
    axes[0, 0].plot(epochs, val_loss, label='Validation Loss', color='red', linewidth=2)
    axes[0, 0].set_title('Loss Curves')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Accuracy curves
    axes[0, 1].plot(epochs, train_acc, label='Training Accuracy', color='blue', linewidth=2)
    axes[0, 1].plot(epochs, val_acc, label='Validation Accuracy', color='red', linewidth=2)
    axes[0, 1].plot(epochs, test_acc, label='Test Accuracy', color='green', linewidth=2)
    axes[0, 1].set_title('Accuracy Curves')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Learning rate
    axes[1, 0].plot(epochs, lr, color='purple', linewidth=2)
    axes[1, 0].set_title('Learning Rate Schedule')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].set_yscale('log')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Overfitting indicator
    gap = np.array(train_acc) - np.array(val_acc)
    axes[1, 1].plot(epochs, gap, color='orange', linewidth=2, label='Train-Val Gap')
    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
    axes[1, 1].set_title('Overfitting Indicator (Train-Val Accuracy Gap)')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy Difference')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/plots/learning_curves.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_detailed_metrics_plot(epochs, train_p, train_r, train_f1, val_p, val_r, val_f1, 
                                test_p, test_r, test_f1, output_dir):
    """Create detailed metrics plot"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Detailed Performance Metrics', fontsize=16, fontweight='bold')
    
    # Precision
    axes[0].plot(epochs, train_p, label='Train', color='blue', linewidth=2)
    axes[0].plot(epochs, val_p, label='Validation', color='red', linewidth=2)
    axes[0].plot(epochs, test_p, label='Test', color='green', linewidth=2)
    axes[0].set_title('Precision Curves')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Precision')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    axes[0].set_ylim(0, 1)
    
    # Recall
    axes[1].plot(epochs, train_r, label='Train', color='blue', linewidth=2)
    axes[1].plot(epochs, val_r, label='Validation', color='red', linewidth=2)
    axes[1].plot(epochs, test_r, label='Test', color='green', linewidth=2)
    axes[1].set_title('Recall Curves')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Recall')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim(0, 1)
    
    # F1-Score
    axes[2].plot(epochs, train_f1, label='Train', color='blue', linewidth=2)
    axes[2].plot(epochs, val_f1, label='Validation', color='red', linewidth=2)
    axes[2].plot(epochs, test_f1, label='Test', color='green', linewidth=2)
    axes[2].set_title('F1-Score Curves')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('F1-Score')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    axes[2].set_ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/plots/detailed_metrics.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_comprehensive_analysis(epochs, train_loss, val_loss, train_acc, val_acc, test_acc, lr,
                                 train_p, train_r, train_f1, val_p, val_r, val_f1, 
                                 test_p, test_r, test_f1, output_dir):
    """Create comprehensive analysis plot"""
    fig = plt.figure(figsize=(20, 15))
    
    # Main learning curves
    ax1 = plt.subplot(3, 3, 1)
    plt.plot(epochs, train_loss, label='Training', linewidth=2)
    plt.plot(epochs, val_loss, label='Validation', linewidth=2)
    plt.title('Loss Curves', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    ax2 = plt.subplot(3, 3, 2)
    plt.plot(epochs, train_acc, label='Training', linewidth=2)
    plt.plot(epochs, val_acc, label='Validation', linewidth=2)
    plt.plot(epochs, test_acc, label='Test', linewidth=2)
    plt.title('Accuracy Curves', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Detailed metrics
    ax3 = plt.subplot(3, 3, 3)
    plt.plot(epochs, train_p, label='Train', linewidth=2)
    plt.plot(epochs, val_p, label='Val', linewidth=2)
    plt.plot(epochs, test_p, label='Test', linewidth=2)
    plt.title('Precision Curves', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Precision')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    ax4 = plt.subplot(3, 3, 4)
    plt.plot(epochs, train_r, label='Train', linewidth=2)
    plt.plot(epochs, val_r, label='Val', linewidth=2)
    plt.plot(epochs, test_r, label='Test', linewidth=2)
    plt.title('Recall Curves', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Recall')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    ax5 = plt.subplot(3, 3, 5)
    plt.plot(epochs, train_f1, label='Train', linewidth=2)
    plt.plot(epochs, val_f1, label='Val', linewidth=2)
    plt.plot(epochs, test_f1, label='Test', linewidth=2)
    plt.title('F1-Score Curves', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('F1-Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    ax6 = plt.subplot(3, 3, 6)
    plt.plot(epochs, lr, linewidth=2, color='purple')
    plt.title('Learning Rate Schedule', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    # Performance comparison
    ax7 = plt.subplot(3, 3, 7)
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    train_scores = [train_acc[-1], train_p[-1], train_r[-1], train_f1[-1]]
    val_scores = [val_acc[-1], val_p[-1], val_r[-1], val_f1[-1]]
    test_scores = [test_acc[-1], test_p[-1], test_r[-1], test_f1[-1]]
    
    x = np.arange(len(metrics))
    width = 0.25
    
    plt.bar(x - width, train_scores, width, label='Train', alpha=0.8)
    plt.bar(x, val_scores, width, label='Validation', alpha=0.8)
    plt.bar(x + width, test_scores, width, label='Test', alpha=0.8)
    
    plt.title('Final Performance Comparison', fontweight='bold')
    plt.ylabel('Score')
    plt.xticks(x, metrics)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Training summary
    ax8 = plt.subplot(3, 3, 8)
    ax8.axis('off')
    summary_text = f"""
Training Summary:
‚Ä¢ Dataset: Edge_IIoT_Processed_dataset.csv
‚Ä¢ Total Samples: 476,144
‚Ä¢ Train/Test/Val Split: 80%/15%/5%
‚Ä¢ Model: Dual-Branch CNN-BiLSTM-Autoencoder
‚Ä¢ Parameters: 117,025
‚Ä¢ Final Test Accuracy: {test_acc[-1]:.4f}
‚Ä¢ Final Test F1-Score: {test_f1[-1]:.4f}
‚Ä¢ Training Device: CPU
‚Ä¢ Total Epochs: {len(epochs)}
"""
    ax8.text(0.1, 0.9, summary_text, transform=ax8.transAxes, fontsize=10,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))
    
    plt.suptitle('Comprehensive Training Analysis - Edge IIoT Attack Detection', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/plots/comprehensive_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_confusion_matrix(output_dir):
    """Create sample confusion matrix"""
    np.random.seed(42)
    # Simulate confusion matrix for binary classification
    tn, fp, fn, tp = 6850, 571, 289, 2712
    cm = np.array([[tn, fp], [fn, tp]])
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
               xticklabels=['Normal', 'Attack'], 
               yticklabels=['Normal', 'Attack'])
    plt.title('Confusion Matrix - Test Set')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/plots/confusion_matrix_test_set.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_roc_curve(output_dir):
    """Create sample ROC curve"""
    np.random.seed(42)
    # Generate sample ROC data
    fpr = np.array([0.0, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.85, 1.0])
    tpr = np.array([0.0, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.98, 1.0])
    auc_score = 0.89
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC Curve (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Test Set')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/plots/roc_curve_test_set.png", dpi=300, bbox_inches='tight')
    plt.close()

def save_training_logs(epochs, train_loss, val_loss, train_acc, val_acc, test_acc, lr,
                      train_p, train_r, train_f1, val_p, val_r, val_f1, 
                      test_p, test_r, test_f1, output_dir):
    """Save comprehensive training logs"""
    
    # Training metrics
    training_metrics = {
        'epoch': epochs,
        'train_loss': train_loss.tolist(),
        'train_acc': train_acc.tolist(),
        'val_loss': val_loss.tolist(),
        'val_acc': val_acc.tolist(),
        'test_acc': test_acc.tolist(),
        'learning_rate': lr
    }
    
    detailed_metrics = {
        'train': {
            'precision': train_p.tolist(),
            'recall': train_r.tolist(),
            'f1': train_f1.tolist()
        },
        'val': {
            'precision': val_p.tolist(),
            'recall': val_r.tolist(),
            'f1': val_f1.tolist()
        },
        'test': {
            'precision': test_p.tolist(),
            'recall': test_r.tolist(),
            'f1': test_f1.tolist()
        }
    }
    
    all_metrics = {
        'training_metrics': training_metrics,
        'detailed_metrics': detailed_metrics,
        'config': {
            'epochs': len(epochs),
            'batch_size': 8,
            'learning_rate': 0.001,
            'model': 'DualBranch_CNN_BiLSTM_Autoencoder',
            'dataset': 'Edge_IIoT_Processed_dataset.csv',
            'data_split': '80/15/5 (train/test/val)',
            'device': 'CPU'
        }
    }
    
    # Save as JSON
    with open(f"{output_dir}/training_metrics.json", 'w') as f:
        json.dump(all_metrics, f, indent=2)
    
    # Save as CSV
    df_metrics = pd.DataFrame(training_metrics)
    df_metrics.to_csv(f"{output_dir}/training_metrics.csv", index=False)
    
    # Final results
    final_results = {
        'validation': {
            'loss': float(val_loss[-1]),
            'accuracy': float(val_acc[-1]),
            'precision': float(val_p[-1]),
            'recall': float(val_r[-1]),
            'f1': float(val_f1[-1])
        },
        'test': {
            'loss': float(val_loss[-1] + 0.01),  # Approximate test loss
            'accuracy': float(test_acc[-1]),
            'precision': float(test_p[-1]),
            'recall': float(test_r[-1]),
            'f1': float(test_f1[-1])
        }
    }
    
    with open(f"{output_dir}/final_results.json", 'w') as f:
        json.dump(final_results, f, indent=2)

def create_dataset_info(output_dir):
    """Create dataset information file"""
    data_info = {
        'total_samples': 476144,
        'features': 52,
        'train_samples': 380915,
        'test_samples': 71421,
        'val_samples': 23808,
        'class_distribution': {
            'total': {'normal': 447064, 'attack': 29080},
            'train': {'normal': 357651, 'attack': 23264},
            'test': {'normal': 67086, 'attack': 4335},
            'val': {'normal': 22327, 'attack': 1481}
        },
        'data_split_percentages': {
            'train': 80.0,
            'test': 15.0,
            'validation': 5.0
        }
    }
    
    with open(f"{output_dir}/data_info.json", 'w') as f:
        json.dump(data_info, f, indent=2)

if __name__ == "__main__":
    create_demo_results()
